
## 1. Environment Configuration

This module was developed and tested under **Python 3.10.19** with **cuda 12.8**. First, execute the following commands to create an environment named `worldarena_embodied` and activate it:
```bash
conda create -n worldarena_embodied python=3.10.19 -y
conda activate worldarena_embodied
```

Next, install PyTorch 2.9.1 (CUDA 12.8 version) compatible with NVIDIA GPUs to ensure the correct training and inference environment:

```bash
pip install torch==2.9.1 torchvision==0.24.1 torchaudio==2.9.1 --index-url https://download.pytorch.org/whl/cu128
```

Then install flash-attn==2.8.3:

```bash
pip install flash-attn==2.8.3 --no-build-isolation
```

Finally, install the remaining dependencies (such as Transformers, OpenCV, etc.):

```bash
cd embodied_task
pip install -r requirements.txt
```

## 2. Training Weights
We fine-tuned the [Wan2.2](https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B) model on the Robotwin2.0 dataset. The trained video model weights and action planning part weights have been uploaded to HuggingFace: https://huggingface.co/WorldArena/WorldArena/tree/main/models \
Download the [CLIP model](https://huggingface.co/openai/clip-vit-base-patch32) and place it under the ./models \
Download the [Wan2.2 model](https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B) and place it under the ./models


The file structure is:
```bash
models/
├── wan_video/ 
│ └── wan_video.pt
├── wan_adjust_bottle/ 
│ └── wan_adjust_bottle.pt
└── wan_click_bell/ 
│ └── wan_click_bell.pt
├── clip-vit-base-patch32/ 
└── Wan2.2-TI2V-5B/ 
```


## 3. Action Planning Task 
First, the model preprocesses the input data by running the preprocessing script.

1.  **Modify Shell Script:**
    *   File path：`./sripts/step1_prepare_latent_wan.sh` `./sripts/generate_metadata.py`
    *   Modify the variable `DATASET_PATH`, replacing its value with **your dataset path**.
    *   Modify the variable `OUTPUT_DIR`， replacing its value with the directory path where you want to store **output files**.

2.  **Modify Python Script:**
    *   File path：`step1_prepare_latent_wan.py`
    *   Modify the variable `ROOT`, replacing its value with **your dataset path**.

After completing the above path replacements, run the preprocessing script.

```bash
python ./scripts/generate_metadata.py
bash ./sripts/step1_prepare_latent_wan.sh
```

Note: Remember to change the `allow_task` variable in `step1_prepare_latent_wan.py` to `adjust_bottle` or `click_bell` depending on the task you want to process. \
Training script path:
```bash
bash ./sripts/train_wan.sh
```

## 4. Data Engine Task

1.  **Edit Shell Script:**：
    *   File path：`evaluate_wan_single.sh` 
    *   Modify the variable `DATASET_PATH`, replacing its value with **your dataset path**.
    *   Modify the variable `OUTPUT_DIR`，replacing its value with the directory path where you want to store **output files**.
2. **Data Format Generated by the Data Engine:**
    * The `first_frames` folder contains the first frames of the episodes.
    * The `video` folder contains the generated action annotations and other annotation information (such as text, paths to the first frames, etc.).
```bash
OUTPUT_DIR/
├── task/ 
│ └── aloha-agilex_clean_50
│   └── aloha-agilex_clean_50
│       └── first_frames
│       └── video
```
Run the data engine generation script.
```bash
bash ./sripts/evaluate_wan_single.sh
```
